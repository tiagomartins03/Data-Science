{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de Modelo e Regressão \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC\n",
    "\n",
    "---\n",
    "\n",
    "Em alguns problemas, podem-se usar muitas variáveis como preditoras em uma regressão. No entanto, adicionar mais variáveis não significa necessariamente que teremos um modelo melhor. Os estatísticos usam o princípio da _navalha de Occam_  para guiar a escolha de um modelo. \n",
    "\n",
    "Incluir variáveis adicionar sempre reduz o RMSE e aumenta o R^2 como observado na métrica AIC (Akaike's Information Criteria). \n",
    "\n",
    "No contexto da **regressão linear**, ao ajustar modelos diferentes, o AIC ajuda a escolher aquele que melhor equilibra a complexidade e o ajuste aos dados. Modelos com valores mais baixos de AIC são preferíveis, pois indicam um melhor ajuste relativo, penalizando menos pela complexidade.\n",
    "\n",
    "AIC = 2P+nlog(RSS/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC: 54.55250860629448\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Dados de exemplo\n",
    "X = np.random.rand(100, 3)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "# Adicionar constante (intercepto) ao modelo\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Ajustar o modelo de regressão linear\n",
    "modelo = sm.OLS(y, X).fit()\n",
    "\n",
    "# Exibir o valor do AIC\n",
    "print('AIC:', modelo.aic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Seleção Progressiva (Forward Selection)**\n",
    "---\n",
    "\n",
    "A seleção progressiva começa com um modelo simples, sem variáveis, e vai adicionando variáveis uma a uma. A cada passo, a variável que mais melhora o critério de ajuste (como AIC, BIC, ou R2) é incluída no modelo.\n",
    "\n",
    "**Passos:**\n",
    "\n",
    "*   Começa sem variáveis no modelo.\n",
    "*   Adiciona a variável que mais melhora o critério de ajuste.\n",
    "*   Repete o processo até que nenhuma variável adicional melhore significativamente o critério."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variáveis Selecionadas: ['var4', 'var1', 'var5']\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.168\n",
      "Model:                            OLS   Adj. R-squared:                  0.135\n",
      "Method:                 Least Squares   F-statistic:                     5.112\n",
      "Date:                Wed, 02 Oct 2024   Prob (F-statistic):            0.00283\n",
      "Time:                        22:26:52   Log-Likelihood:                -9.0609\n",
      "No. Observations:                  80   AIC:                             26.12\n",
      "Df Residuals:                      76   BIC:                             35.65\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4969      0.086      5.796      0.000       0.326       0.668\n",
      "var4           0.3840      0.110      3.504      0.001       0.166       0.602\n",
      "var1          -0.2078      0.104     -1.991      0.050      -0.416    5.63e-05\n",
      "var5          -0.1848      0.102     -1.808      0.075      -0.389       0.019\n",
      "==============================================================================\n",
      "Omnibus:                        3.545   Durbin-Watson:                   2.032\n",
      "Prob(Omnibus):                  0.170   Jarque-Bera (JB):                2.054\n",
      "Skew:                          -0.129   Prob(JB):                        0.358\n",
      "Kurtosis:                       2.259   Cond. No.                         5.45\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gerar dados de exemplo\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame(np.random.rand(100, 5), columns=['var1', 'var2', 'var3', 'var4', 'var5'])\n",
    "y = pd.Series(np.random.rand(100))\n",
    "\n",
    "# Adicionar constante ao modelo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Função de Seleção Progressiva (Forward Selection)\n",
    "def forward_selection(X, y):\n",
    "    initial_features = []\n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    best_aic = np.inf\n",
    "    \n",
    "    while remaining_features:\n",
    "        aics_with_candidates = []\n",
    "        \n",
    "        for candidate in remaining_features:\n",
    "            # Modelo com as features já selecionadas + candidata\n",
    "            features = initial_features + [candidate]\n",
    "            X_with_candidate = sm.add_constant(X[features])\n",
    "            model = sm.OLS(y, X_with_candidate).fit()\n",
    "            aics_with_candidates.append((model.aic, candidate))\n",
    "        \n",
    "        # Seleciona o modelo com menor AIC\n",
    "        aic, best_candidate = min(aics_with_candidates, key=lambda x: x[0])\n",
    "        \n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            initial_features.append(best_candidate)\n",
    "            remaining_features.remove(best_candidate)\n",
    "            selected_features.append(best_candidate)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Executar Seleção Progressiva\n",
    "best_features = forward_selection(X_train, y_train)\n",
    "\n",
    "# Exibir as melhores variáveis selecionadas\n",
    "print(\"Variáveis Selecionadas:\", best_features)\n",
    "\n",
    "# Ajustar o modelo final com as variáveis selecionadas\n",
    "X_selected = sm.add_constant(X_train[best_features])\n",
    "modelo_final = sm.OLS(y_train, X_selected).fit()\n",
    "\n",
    "# Exibir o sumário do modelo final\n",
    "print(modelo_final.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicação:\n",
    "\n",
    "forward_selection: Essa função itera sobre as variáveis disponíveis, adicionando-as ao modelo uma de cada vez e calculando o AIC. A cada iteração, a variável que mais reduz o AIC é mantida, até que nenhuma adição de variável melhore o modelo.\n",
    "\n",
    "Modelo Final: Após a seleção progressiva, o modelo final é ajustado com as variáveis selecionadas e o sumário estatístico é exibido.\n",
    "\n",
    "Notas:\n",
    "\n",
    "O critério de seleção usado aqui é o AIC.\n",
    "\n",
    "A função seleciona variáveis até que nenhuma outra adição melhore o valor do AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Seleção Regressiva (Backward Elimination)**\n",
    "---\n",
    "\n",
    "Na seleção regressiva, o processo é o oposto: começa-se com todas as variáveis no modelo, e a cada passo, remove-se a variável menos significativa, ou aquela que mais prejudica o critério de ajuste.\n",
    "\n",
    "**Passos:**\n",
    "\n",
    "*   Começa com todas as variáveis no modelo.\n",
    "*   Remove a variável menos significativa (maior p-valor ou que piora o critério de ajuste).\n",
    "*   Repete o processo até que nenhuma variável possa ser removida sem piorar significativamente o critério."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo variável var3 com p-valor 0.8251818703813887\n",
      "Removendo variável var2 com p-valor 0.45466654280745067\n",
      "Removendo variável var5 com p-valor 0.07460578207405656\n",
      "Removendo variável var1 com p-valor 0.07584286597619241\n",
      "Variáveis Selecionadas: ['var4']\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.096\n",
      "Model:                            OLS   Adj. R-squared:                  0.084\n",
      "Method:                 Least Squares   F-statistic:                     8.247\n",
      "Date:                Wed, 02 Oct 2024   Prob (F-statistic):            0.00525\n",
      "Time:                        22:29:05   Log-Likelihood:                -12.393\n",
      "No. Observations:                  80   AIC:                             28.79\n",
      "Df Residuals:                      78   BIC:                             33.55\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3462      0.064      5.397      0.000       0.219       0.474\n",
      "var4           0.3125      0.109      2.872      0.005       0.096       0.529\n",
      "==============================================================================\n",
      "Omnibus:                        9.250   Durbin-Watson:                   1.921\n",
      "Prob(Omnibus):                  0.010   Jarque-Bera (JB):                3.567\n",
      "Skew:                          -0.194   Prob(JB):                        0.168\n",
      "Kurtosis:                       2.041   Cond. No.                         4.35\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gerar dados de exemplo\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame(np.random.rand(100, 5), columns=['var1', 'var2', 'var3', 'var4', 'var5'])\n",
    "y = pd.Series(np.random.rand(100))\n",
    "\n",
    "# Adicionar constante ao modelo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Função de Seleção Regressiva (Backward Elimination)\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    features = list(X.columns)\n",
    "    \n",
    "    while len(features) > 0:\n",
    "        X_with_constant = sm.add_constant(X[features])\n",
    "        model = sm.OLS(y, X_with_constant).fit()\n",
    "        p_values = model.pvalues.iloc[1:]  # Excluir o p-valor do intercepto (const)\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        if max_p_value > significance_level:\n",
    "            # Remove a variável com o maior p-valor\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "            print(f\"Removendo variável {excluded_feature} com p-valor {max_p_value}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Executar Seleção Regressiva\n",
    "best_features = backward_elimination(X_train, y_train)\n",
    "\n",
    "# Exibir as melhores variáveis selecionadas\n",
    "print(\"Variáveis Selecionadas:\", best_features)\n",
    "\n",
    "# Ajustar o modelo final com as variáveis selecionadas\n",
    "X_selected = sm.add_constant(X_train[best_features])\n",
    "modelo_final = sm.OLS(y_train, X_selected).fit()\n",
    "\n",
    "# Exibir o sumário do modelo final\n",
    "print(modelo_final.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicação:\n",
    "\n",
    "backward_elimination: A função ajusta um modelo de regressão linear com todas as variáveis e, em seguida, remove a variável com o maior p-valor (menos significativa). O processo continua até que todas as variáveis restantes tenham um p-valor menor que o nível de significância fornecido (neste caso, 0.05).\n",
    "\n",
    "Critério de exclusão: A variável com o maior p-valor é removida em cada iteração, até que todas as variáveis no modelo sejam estatisticamente significativas.\n",
    "\n",
    "Modelo Final: Depois da eliminação regressiva, o modelo final é ajustado com as variáveis restantes e o sumário estatístico do modelo é exibido.\n",
    "\n",
    "Resultado:\n",
    "\n",
    "Durante a execução, o código exibirá as variáveis sendo removidas e, no final, o modelo será ajustado apenas com as variáveis selecionadas.\n",
    "\n",
    "Esse método é útil para remover variáveis irrelevantes e melhorar a interpretabilidade do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Penalizada\n",
    "---\n",
    "\n",
    "A regressão penalizada é uma técnica mais sofisticada que envolve a adição de um termo de penalidade à função de ajuste, para evitar overfitting e selecionar variáveis automaticamente. As formas mais comuns de regressão penalizada são **Ridge** e **Lasso**.\n",
    "\n",
    "- **Ridge Regression** adiciona uma penalidade proporcional ao quadrado dos coeficientes. Ela reduz os coeficientes, mas não os zera. A fórmula penalizada da função de custo é:\n",
    "\n",
    " $$\\text{Custo Ridge} = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum \\beta_j^2$$\n",
    "\n",
    "- **Lasso Regression** (Least Absolute Shrinkage and Selection Operator) adiciona uma penalidade proporcional ao valor absoluto dos coeficientes. Além de reduzir, o Lasso pode zerar alguns coeficientes, funcionando como uma técnica de seleção de variáveis:\n",
    "\n",
    " $$\\text{Custo Lasso} = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum |\\beta_j|$$\n",
    "\n",
    " **Elastic Net** combina as penalidades do Ridge e do Lasso, dando um controle maior sobre a seleção de variáveis e a regularização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Lasso): 0.08586156548817339\n",
      "Coeficientes (Lasso): [-0. -0. -0.  0. -0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gerar dados de exemplo\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame(np.random.rand(100, 5), columns=['var1', 'var2', 'var3', 'var4', 'var5'])\n",
    "y = pd.Series(np.random.rand(100))\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajustar o modelo Lasso\n",
    "modelo_lasso = Lasso(alpha=0.1)  # 'alpha' é o parâmetro de regularização\n",
    "modelo_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predição e avaliação\n",
    "y_pred = modelo_lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print('MSE (Lasso):', mse)\n",
    "print('Coeficientes (Lasso):', modelo_lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicação:\n",
    "\n",
    "1.  **Lasso Regression**: O modelo de regressão Lasso é ajustado utilizando o parâmetro `alpha` que controla a força da regularização. Um valor maior de `alpha` penaliza mais os coeficientes, podendo forçar alguns coeficientes a zero, o que implica em uma seleção de variáveis.\n",
    "2.  **Treinamento e Teste**: Os dados são divididos em conjuntos de treinamento e teste para avaliar o modelo.\n",
    "3.  **Coeficientes**: O Lasso pode zerar alguns coeficientes, permitindo que variáveis irrelevantes sejam eliminadas do modelo.\n",
    "4.  **Erro Quadrático Médio (MSE)**: O erro do modelo é calculado com base no conjunto de teste.\n",
    "\n",
    "### Notas:\n",
    "\n",
    "*   O parâmetro `alpha` controla o quanto a penalização impacta o modelo. Valores maiores de `alpha` resultam em mais variáveis sendo eliminadas (coeficientes sendo zerados).\n",
    "*   O `coef_` do modelo exibirá os coeficientes de cada variável após a regularização. Coeficientes zero indicam que a variável foi eliminada pelo Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Ridge): 0.12296817407252772\n",
      "Coeficientes (Ridge): [-0.21544454 -0.07665229 -0.02477832  0.36864494 -0.18638862]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gerar dados de exemplo\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame(np.random.rand(100, 5), columns=['var1', 'var2', 'var3', 'var4', 'var5'])\n",
    "y = pd.Series(np.random.rand(100))\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajustar o modelo Ridge\n",
    "modelo_ridge = Ridge(alpha=0.1)  # 'alpha' é o parâmetro de regularização\n",
    "modelo_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predição e avaliação\n",
    "y_pred = modelo_ridge.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print('MSE (Ridge):', mse)\n",
    "print('Coeficientes (Ridge):', modelo_ridge.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicação:\n",
    "\n",
    "1.  **Ridge Regression**: A técnica de **Ridge** é usada para penalizar os coeficientes grandes, adicionando uma penalidade proporcional ao quadrado dos coeficientes:\n",
    "\n",
    "   $$\n",
    "   \\sum \\beta_j^2\n",
    "   $$\n",
    "\n",
    "   Isso ajuda a reduzir o overfitting.\n",
    "\n",
    "2.  **Parâmetro `alpha`**: O parâmetro `alpha` controla a intensidade da penalização. Quanto maior o valor de `alpha`, maior a penalização, o que tende a reduzir ainda mais os coeficientes.\n",
    "\n",
    "3.  **Divisão dos Dados**: Os dados são divididos em conjuntos de treinamento e teste. O modelo é treinado com o conjunto de treinamento e avaliado com o conjunto de teste.\n",
    "\n",
    "4.  **Erro Quadrático Médio (MSE)**: O erro do modelo é calculado no conjunto de teste para avaliar o desempenho.\n",
    "\n",
    "5.  **Coeficientes**: Os coeficientes do modelo Ridge são exibidos. A penalização pode reduzir os coeficientes em magnitude, mas normalmente não os zera completamente como no Lasso.\n",
    "\n",
    "### Notas:\n",
    "\n",
    "- O valor de `alpha` deve ser ajustado com base na quantidade de regularização desejada. Um valor muito pequeno pode resultar em overfitting, enquanto um valor muito grande pode prejudicar o ajuste ao ponto de ignorar quase todas as variáveis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
